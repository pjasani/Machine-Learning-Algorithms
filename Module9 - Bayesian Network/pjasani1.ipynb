{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "1. Change the name of this file to be your JHED id as in `jsmith299.ipynb`. Because sure you use your JHED ID (it's made out of your name and not your student id which is just letters and numbers).\n",
    "2. Make sure the notebook you submit is cleanly and fully executed. I do not grade unexecuted notebooks.\n",
    "3. Submit your notebook back in Blackboard where you downloaded this file.\n",
    "\n",
    "*Provide the output **exactly** as requested*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Naive Bayes Classifier with the same data from last week:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "(You should have downloaded it).\n",
    "\n",
    "You'll first need to calculate all of the necessary probabilities (don't forget to use +1 smoothing) using a `train` function. You'll then need to have a `classify` function that takes your probabilities, a List of instances (possibly a list of 1) and returns a List of Dicts. Each Dict has a key for every possible class label and the associated *normalized* probability. For example, if we have given the `classify` function a list of 2 observations, we would get the following back:\n",
    "\n",
    "```\n",
    "[{\"e\": 0.98, \"p\": 0.02}, {\"e\": 0.34, \"p\": 0.66}]\n",
    "```\n",
    "\n",
    "when calculating the error rate of your classifier, you should pick the class label with the highest probability; you can write a simple function that takes the Dict and returns that class label.\n",
    "\n",
    "As a reminder, the Naive Bayes Classifier generates the *unnormalized* probabilities from the numerator of Bayes Rule:\n",
    "\n",
    "$$P(C|A) \\propto P(A|C)P(C)$$\n",
    "\n",
    "where C is the class and A are the attributes (data). Since the normalizer of Bayes Rule is the *sum* of all possible numerators and you have to calculate them all, the normalizer is just the sum of the probabilities.\n",
    "\n",
    "You'll also need an `evaluate` function as before. You should use the $error\\_rate$ again.\n",
    "\n",
    "With +1 smoothing, the Naive Bayes Classifier has quite a different way of handling missing values.\n",
    "\n",
    "\n",
    "Again, you must implement the following functions:\n",
    "\n",
    "`cross_validate` takes the data and performs 10 fold cross validation.\n",
    "\n",
    "`train` takes training_data and returns the probabilities as a data structure. If some kind of ADT seems reasonable to you, then you can create one but you don't really need one. Nested Dicts will work just fine.\n",
    "\n",
    "`classify` takes the probabilities produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data).\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "\n",
    "```\n",
    "def train(training_data):\n",
    "   # returns a NBC probability structure\n",
    "```\n",
    "\n",
    "and `classify` takes probabilities and a List of instances (possibly just one) and returns the classifications:\n",
    "\n",
    "```\n",
    "def classify(probabilities, test_data):\n",
    "    # returns a list of classifications\n",
    "```\n",
    "\n",
    "and `evaluate` takes the actual classifications and the predicted classes and returns the classification error rate:\n",
    "\n",
    "```\n",
    "def evaluate(actual, predicted):\n",
    "    # returns an error rate\n",
    "```\n",
    "\n",
    "You must apply 10 fold cross validation to your data set. You will treat each fold as a test set, using the combined remainder as the training set. You should print out the error rate for each fold and then an average error rate for the entire cross validation process. Format the error rate as a percent (2.34% not 0.0234).\n",
    "\n",
    "This is all that is required for this assignment. I'm leaving more of the particulars up to you but you can definitely use the last module as a guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load_file**<br>\n",
    "The `load_file` load the file given the filename.\n",
    "\n",
    "Parameters:\n",
    "* **file_name** is the name of the file we want to load.\n",
    "\n",
    "The function return the data loaded from the file in `list of lists` format.</br>\n",
    "For example if the data file contrains\n",
    "```\n",
    "        0,1,2\n",
    "        2,3,4\n",
    "```\n",
    "retuns:<br>\n",
    "`[[0,1,2], [2,3,4]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        tmp_data = line.rstrip().split(\",\")\n",
    "        data += [tmp_data[1:] + tmp_data[:1]] # made sure the class column was the last one\n",
    "        # np.append(data, np.array(tmp_data[1:] + tmp_data[:1])) # made sure the class column was the last one\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filter_data**<br>\n",
    "The `filter_data` is a helper function for `id3 algorithm`. It creates a new list of data that maches that condition that attribute = attribute value.\n",
    "\n",
    "Parameters:\n",
    "* **data** is the data we want to filter.\n",
    "* **feature** the feature we want to usee for the filter.\n",
    "* **feature_val** is the data value we want to filter the data with.\n",
    "\n",
    "It returns `filtered data` in list of lists format.<br>\n",
    "For data contrains below where columns are features, feature = 1 and feature_val = 1\n",
    "```\n",
    "        0,1,2\n",
    "        2,5,4\n",
    "        3,1,2\n",
    "```\n",
    "\n",
    "retuns:<br>\n",
    "`[[0,1,2], [3,1,2]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data, feature, feature_val):\n",
    "    return data[np.where(data[:,feature] == feature_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train**<br>\n",
    "The `train` is the function that calulates the probability of the class labels and the features and save it in a dictionary format.\n",
    "\n",
    "Parameters:\n",
    "* **data** is the data used to train.\n",
    "\n",
    "It returns a `dictionary` containing features and probabilites for each class label.<br>\n",
    "For example,  if given data is as shown below where last column is class label\n",
    "```\n",
    "        2,5,p\n",
    "        3,1,e\n",
    "```\n",
    "\n",
    "returns:<br>\n",
    "```\n",
    "{\n",
    "    0: {2:{{e:0.5 , p:0.5 }}, 3:{{e:0.5 , p:0.5 }}},\n",
    "    1: {1:{{e:0.5 , p:0.5 }}, 5:{{e:0.5 , p:0.5 }}},\n",
    "    p: 0.5,\n",
    "    e: 0.5\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data):\n",
    "    nbc_prob = {}    \n",
    "    count = {class_label:data[:,-1].tolist().count(class_label) for class_label in list(set(data[:,-1]))}\n",
    "\n",
    "    for col in range(data.shape[1]-1):\n",
    "        nbc_prob[col] = {}\n",
    "        for domain_val in set(data[:,col]):\n",
    "            subset = filter_data(data, col, domain_val)\n",
    "            nbc_prob[col][domain_val] = {}\n",
    "            for class_label in set(data[:,-1]):\n",
    "                nbc_prob[col][domain_val][class_label] = (subset[:,-1].tolist().count(class_label)+1) / (count[class_label] + 1)\n",
    "    for clss in count: # add class probabilities\n",
    "        nbc_prob[clss] = count[clss]/len(data)\n",
    "    return nbc_prob   # returns a NBC probability structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**normalize**<br>\n",
    "The `normalize` is helper function for `classify`. It takes the dictionary of class label probabilites as an argument and calculates normalized probabilities.\n",
    "\n",
    "Parameters:\n",
    "* **dictionary** is the class label probabilites we want to normalize.\n",
    "\n",
    "It returns a `dictionary` of of normalized probabilities.<br>\n",
    "\n",
    "For example, if the dictionary is <br>\n",
    "`{e:0.2 , p:0.6}`\n",
    "\n",
    "retuns:<br>\n",
    "`{e:0.25 , p:0.75}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dictionary):\n",
    "    probs = dictionary.copy()\n",
    "    total_prob = sum(probs.values()) \n",
    "    for key in probs:\n",
    "        probs[key] /= total_prob\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**classify**<br>\n",
    "The `classify` function takes the test data and find calculates a label for each testpoint using the calculated probabilities. It then calculates probability each testpoint normalizes it and then finds the label for each testpoint.\n",
    "\n",
    "Parameters:\n",
    "* **nbc_prob** is dictionary of calculated probabilites for each feature.\n",
    "* **test_data** is the test points we want to predict the class label.\n",
    "\n",
    "It returns tuple of predicted `class labels` and list of `normalized probabilities` for each testpoint.<br>\n",
    "\n",
    "For Example, if nbc_prob is as shown below and test data is [[2,1], [3,5]]<br>\n",
    "```\n",
    "{\n",
    "    0: {2:{{e:0.22 , p:0.6 }}, 3:{{e:0.8 , p:0.1 }}},\n",
    "    1: {1:{{e:0.3 , p:0.2 }}, 5:{{e:0.9 , p:0.05 }}},\n",
    "    p: 0.4,\n",
    "    e: 0.6\n",
    "}\n",
    "```\n",
    "retuns:<br>\n",
    "`([p, e], [{e:0.4520 , p:0.5479 },{e:0.995 , p:0.005}])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(nbc_prob, test_data):\n",
    "    # get the total possible class labels\n",
    "    result_list = []\n",
    "    class_labels = [key for key in nbc_prob if type(key) != int]  # get the class labels\n",
    "    for testpoint in test_data:\n",
    "        result = {}\n",
    "        for c_label in class_labels:\n",
    "            result[c_label] = nbc_prob[c_label]    # initialize probability\n",
    "            for col in range(len(testpoint)):\n",
    "                result[c_label] *= nbc_prob[col][testpoint[col]][c_label] # multiply by probability of each feature\n",
    "        result_list += [result]\n",
    "    \n",
    "    norm_probs = [normalize(result) for result in result_list] # normalize\n",
    "    label = [max(rst, key = lambda x: rst[x]) for rst in norm_probs] # find the label\n",
    "    return (label, norm_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**evaluate**<br>\n",
    "The `evaluate` function that calculates the error rate between the predicted and the actual. The below equation is used to calculate the erorr rate.\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Parameters:\n",
    "* **actual** is the actual test label for the each test observation.\n",
    "* **predicted** is the class labels that were predicted by our model.\n",
    "\n",
    "It returns the `error rate`.<br>\n",
    "\n",
    "For example, if actual = [e,e,e,p], predicted = [e,p,e,p]<br>\n",
    "\n",
    "returns:<br>\n",
    "`0.25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(actual, predicted):\n",
    "    count = np.where(np.array(actual) != np.array(predicted))[0]\n",
    "    error_rate = len(count)/len(actual)\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cross_validate**<br>\n",
    "The `cross_validate` check the accuracy of our model by running the 10 fold cross validation. It prints the error rate of each fold and the averate error rate of 10 folds.\n",
    "\n",
    "Parameters:\n",
    "* **data** is data we want to use for cross validation.\n",
    "* **cross_validate** indicates what percent(as decimal) of the data we want to use to train the model.\n",
    "\n",
    "retuns:<br>\n",
    "It does not return anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data, train_percent=.8):\n",
    "    errors = []\n",
    "    for test_nun in range(10):\n",
    "        train_data = random.sample(data.tolist(), int(len(data)*train_percent))\n",
    "        test_data = [row[:-1] for row in data.tolist() if row not in train_data]\n",
    "        test_label = [row[-1] for row in data.tolist() if row not in train_data]\n",
    "        train_data = np.array(train_data)\n",
    "        test_data = np.array(test_data)\n",
    "        tree = train(train_data)\n",
    "        predicted, result_prob = classify(tree, test_data)\n",
    "        error_rate = evaluate(test_label, predicted)\n",
    "        errors += [error_rate]\n",
    "        print(f\"Fold# {test_nun+1:2}: Error Rate: {round(error_rate*100,3)}%\")\n",
    "    print(f\"\\nAverage Error Rate: {round((sum(errors)/ len(errors))*100, 3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold#  1: Error Rate: 5.231%\n",
      "Fold#  2: Error Rate: 4.369%\n",
      "Fold#  3: Error Rate: 4.123%\n",
      "Fold#  4: Error Rate: 4.431%\n",
      "Fold#  5: Error Rate: 4.308%\n",
      "Fold#  6: Error Rate: 4.862%\n",
      "Fold#  7: Error Rate: 5.969%\n",
      "Fold#  8: Error Rate: 6.215%\n",
      "Fold#  9: Error Rate: 3.938%\n",
      "Fold# 10: Error Rate: 5.538%\n",
      "\n",
      "Average Error Rate: 4.898%\n"
     ]
    }
   ],
   "source": [
    "#read the file\n",
    "data = load_file(\"agaricus-lepiota.data\")\n",
    "cross_validate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en605645)",
   "language": "python",
   "name": "en605645"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
